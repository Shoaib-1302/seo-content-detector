{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bTY1M7HfGiHk",
        "outputId": "24e644b3-d1b8-4e03-f8c6-0fc2f7ebdc1d"
      },
      "outputs": [],
      "source": [
        "# SEO Content Quality & Duplicate Detector - Complete Pipeline\n",
        "# Data Science Assignment\n",
        "\n",
        "# ============================================================================\n",
        "# SECTION 1: IMPORTS AND SETUP\n",
        "# ============================================================================\n",
        "%pip install textstat sentence-transformers beautifulsoup4 requests scikit-learn matplotlib seaborn\n",
        "%pip install --upgrade pandas numpy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import json\n",
        "import warnings\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from time import sleep\n",
        "from urllib.parse import urlparse\n",
        "import os\n",
        "\n",
        "# NLP and Feature Engineering\n",
        "import textstat\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RzdVud5HSVI"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 2: DATA LOADING AND HTML PARSING\n",
        "# ============================================================================\n",
        "\n",
        "def parse_html_content(html_content):\n",
        "    \"\"\"\n",
        "    Parse HTML content and extract meaningful text.\n",
        "\n",
        "    Args:\n",
        "        html_content (str): Raw HTML content\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary with title and body_text\n",
        "    \"\"\"\n",
        "    try:\n",
        "        soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
        "            script.decompose()\n",
        "\n",
        "        # Extract title\n",
        "        title = soup.title.string if soup.title else \"\"\n",
        "        title = title.strip() if title else \"\"\n",
        "\n",
        "        # Extract body text from main content areas\n",
        "        body_text = \"\"\n",
        "\n",
        "        # Priority order for content extraction\n",
        "        content_tags = soup.find_all(['article', 'main', 'div', 'p'])\n",
        "\n",
        "        if content_tags:\n",
        "            body_text = ' '.join([tag.get_text() for tag in content_tags])\n",
        "        else:\n",
        "            body_text = soup.get_text()\n",
        "\n",
        "        # Clean the text\n",
        "        body_text = re.sub(r'\\s+', ' ', body_text).strip()\n",
        "\n",
        "        return {\n",
        "            'title': title,\n",
        "            'body_text': body_text\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing HTML: {str(e)}\")\n",
        "        return {\n",
        "            'title': \"\",\n",
        "            'body_text': \"\"\n",
        "        }\n",
        "\n",
        "def scrape_url(url, timeout=10):\n",
        "    \"\"\"\n",
        "    Scrape a single URL and return HTML content.\n",
        "\n",
        "    Args:\n",
        "        url (str): URL to scrape\n",
        "        timeout (int): Request timeout in seconds\n",
        "\n",
        "    Returns:\n",
        "        str: HTML content or empty string on failure\n",
        "    \"\"\"\n",
        "    try:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "        response = requests.get(url, headers=headers, timeout=timeout)\n",
        "        response.raise_for_status()\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\"Error scraping {url}: {str(e)}\")\n",
        "        return \"\"\n",
        "\n",
        "# Load the dataset\n",
        "print(\"Loading dataset...\")\n",
        "df = pd.read_csv('data/data.csv')\n",
        "print(f\"Dataset loaded: {len(df)} rows\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "\n",
        "# Parse HTML content or scrape if needed\n",
        "print(\"\\nParsing HTML content...\")\n",
        "parsed_data = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    print(f\"Processing row {idx + 1}/{len(df)}\", end='\\r')\n",
        "\n",
        "    url = row['url']\n",
        "\n",
        "    # Check if html_content column exists\n",
        "    if 'html_content' in df.columns and pd.notna(row['html_content']):\n",
        "        # Use pre-scraped HTML content\n",
        "        parsed = parse_html_content(row['html_content'])\n",
        "    else:\n",
        "        # Need to scrape the URL\n",
        "        html_content = scrape_url(url)\n",
        "        if html_content:\n",
        "            parsed = parse_html_content(html_content)\n",
        "            sleep(1.5)  # Rate limiting\n",
        "        else:\n",
        "            parsed = {'title': '', 'body_text': ''}\n",
        "\n",
        "    # Calculate word count\n",
        "    word_count = len(parsed['body_text'].split())\n",
        "\n",
        "    parsed_data.append({\n",
        "        'url': url,\n",
        "        'title': parsed['title'],\n",
        "        'body_text': parsed['body_text'],\n",
        "        'word_count': word_count\n",
        "    })\n",
        "\n",
        "# Create DataFrame with extracted content\n",
        "extracted_df = pd.DataFrame(parsed_data)\n",
        "print(f\"\\n\\nExtracted content from {len(extracted_df)} pages\")\n",
        "print(f\"Average word count: {extracted_df['word_count'].mean():.0f}\")\n",
        "\n",
        "# Create data directory if it doesn't exist\n",
        "if not os.path.exists('data'):\n",
        "    os.makedirs('data')\n",
        "\n",
        "# Save extracted content (without html_content to reduce file size)\n",
        "extracted_df.to_csv('data/extracted_content.csv', index=False)\n",
        "print(\"Saved to: data/extracted_content.csv\")\n",
        "\n",
        "# Display sample\n",
        "print(\"\\nSample extracted content:\")\n",
        "print(extracted_df[['url', 'title', 'word_count']].head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 3: TEXT PREPROCESSING & FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean text by lowercasing and removing extra whitespace.\"\"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "def count_sentences(text):\n",
        "    \"\"\"Count sentences in text.\"\"\"\n",
        "    sentences = re.split(r'[.!?]+', text)\n",
        "    return len([s for s in sentences if s.strip()])\n",
        "\n",
        "def extract_top_keywords(text, n=5):\n",
        "    \"\"\"Extract top N keywords using TF-IDF.\"\"\"\n",
        "    try:\n",
        "        if not text.strip():\n",
        "            return \"\"\n",
        "\n",
        "        vectorizer = TfidfVectorizer(max_features=n, stop_words='english')\n",
        "        tfidf_matrix = vectorizer.fit_transform([text])\n",
        "        feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "        return \"|\".join(feature_names)\n",
        "    except:\n",
        "        return \"\"\n",
        "\n",
        "print(\"Extracting features from text...\")\n",
        "\n",
        "# Initialize sentence transformer for embeddings\n",
        "print(\"Loading sentence transformer model...\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "features_list = []\n",
        "\n",
        "for idx, row in extracted_df.iterrows():\n",
        "    print(f\"Processing features {idx + 1}/{len(extracted_df)}\", end='\\r')\n",
        "\n",
        "    text = row['body_text']\n",
        "    cleaned_text = clean_text(text)\n",
        "\n",
        "    # Basic metrics\n",
        "    word_count = row['word_count']\n",
        "    sentence_count = count_sentences(text)\n",
        "\n",
        "    # Readability score (Flesch Reading Ease)\n",
        "    try:\n",
        "        flesch_score = textstat.flesch_reading_ease(text) if text.strip() else 0\n",
        "    except:\n",
        "        flesch_score = 0\n",
        "\n",
        "    # Top keywords\n",
        "    keywords = extract_top_keywords(cleaned_text)\n",
        "\n",
        "    # Generate embedding\n",
        "    try:\n",
        "        embedding = model.encode(text[:512])  # Limit text length for efficiency\n",
        "    except:\n",
        "        embedding = np.zeros(384)  # Default embedding size for this model\n",
        "\n",
        "    features_list.append({\n",
        "        'url': row['url'],\n",
        "        'word_count': word_count,\n",
        "        'sentence_count': sentence_count,\n",
        "        'flesch_reading_ease': flesch_score,\n",
        "        'top_keywords': keywords,\n",
        "        'embedding': embedding.tolist()\n",
        "    })\n",
        "\n",
        "# Create features DataFrame\n",
        "features_df = pd.DataFrame(features_list)\n",
        "print(f\"\\n\\nFeatures extracted for {len(features_df)} pages\")\n",
        "\n",
        "# Save features to CSV\n",
        "features_df.to_csv('data/features.csv', index=False)\n",
        "print(\"Saved to: data/features.csv\")\n",
        "\n",
        "# Display feature statistics\n",
        "print(\"\\nFeature Statistics:\")\n",
        "print(f\"Word count - Mean: {features_df['word_count'].mean():.0f}, \"\n",
        "      f\"Min: {features_df['word_count'].min()}, Max: {features_df['word_count'].max()}\")\n",
        "print(f\"Sentence count - Mean: {features_df['sentence_count'].mean():.1f}\")\n",
        "print(f\"Flesch Reading Ease - Mean: {features_df['flesch_reading_ease'].mean():.1f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 4: DUPLICATE DETECTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DUPLICATE DETECTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Convert embeddings to numpy array\n",
        "embeddings_array = np.array(features_df['embedding'].tolist())\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "print(\"Computing cosine similarity matrix...\")\n",
        "similarity_matrix = cosine_similarity(embeddings_array)\n",
        "\n",
        "# Set similarity threshold\n",
        "SIMILARITY_THRESHOLD = 0.80\n",
        "\n",
        "# Find duplicate pairs\n",
        "duplicates = []\n",
        "n = len(similarity_matrix)\n",
        "\n",
        "for i in range(n):\n",
        "    for j in range(i + 1, n):\n",
        "        similarity = similarity_matrix[i][j]\n",
        "        if similarity > SIMILARITY_THRESHOLD:\n",
        "            duplicates.append({\n",
        "                'url1': features_df.iloc[i]['url'],\n",
        "                'url2': features_df.iloc[j]['url'],\n",
        "                'similarity': similarity\n",
        "            })\n",
        "\n",
        "duplicates_df = pd.DataFrame(duplicates)\n",
        "\n",
        "# Thin content detection (word count < 500)\n",
        "features_df['is_thin'] = features_df['word_count'] < 500\n",
        "\n",
        "# Save duplicates\n",
        "if len(duplicates_df) > 0:\n",
        "    duplicates_df.to_csv('data/duplicates.csv', index=False)\n",
        "    print(f\"Saved {len(duplicates_df)} duplicate pairs to: data/duplicates.csv\")\n",
        "else:\n",
        "    print(\"No duplicate pairs found above threshold\")\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"DUPLICATE DETECTION SUMMARY\")\n",
        "print(\"-\"*80)\n",
        "print(f\"Total pages analyzed: {len(features_df)}\")\n",
        "print(f\"Duplicate pairs found: {len(duplicates_df)}\")\n",
        "print(f\"Thin content pages: {features_df['is_thin'].sum()} \"\n",
        "      f\"({features_df['is_thin'].sum() / len(features_df) * 100:.1f}%)\")\n",
        "\n",
        "if len(duplicates_df) > 0:\n",
        "    print(\"\\nTop duplicate pairs:\")\n",
        "    print(duplicates_df.sort_values('similarity', ascending=False).head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 5: CONTENT QUALITY SCORING MODEL\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CONTENT QUALITY SCORING\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create synthetic labels based on clear criteria\n",
        "def assign_quality_label(row):\n",
        "    \"\"\"Assign quality label based on word count and readability.\"\"\"\n",
        "    wc = row['word_count']\n",
        "    fre = row['flesch_reading_ease']\n",
        "\n",
        "    if wc > 1500 and 50 <= fre <= 70:\n",
        "        return 'High'\n",
        "    elif wc < 500 or fre < 30:\n",
        "        return 'Low'\n",
        "    else:\n",
        "        return 'Medium'\n",
        "\n",
        "# Assign labels\n",
        "features_df['quality_label'] = features_df.apply(assign_quality_label, axis=1)\n",
        "\n",
        "print(\"Quality label distribution:\")\n",
        "print(features_df['quality_label'].value_counts())\n",
        "\n",
        "# Prepare features for modeling\n",
        "X = features_df[['word_count', 'sentence_count', 'flesch_reading_ease']].values\n",
        "y = features_df['quality_label'].values\n",
        "\n",
        "# Encode labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set: {len(X_train)} samples\")\n",
        "print(f\"Test set: {len(X_test)} samples\")\n",
        "\n",
        "# Train Random Forest model\n",
        "print(\"\\nTraining Random Forest model...\")\n",
        "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Baseline model (rule-based using only word count)\n",
        "def baseline_predict(word_counts):\n",
        "    \"\"\"Simple baseline using word count only.\"\"\"\n",
        "    predictions = []\n",
        "    for wc in word_counts:\n",
        "        if wc > 1500:\n",
        "            predictions.append(le.transform(['High'])[0])\n",
        "        elif wc < 500:\n",
        "            predictions.append(le.transform(['Low'])[0])\n",
        "        else:\n",
        "            predictions.append(le.transform(['Medium'])[0])\n",
        "    return np.array(predictions)\n",
        "\n",
        "y_pred_baseline = baseline_predict(X_test[:, 0])\n",
        "\n",
        "# Evaluate models\n",
        "print(\"\\n\" + \"-\"*80)\n",
        "print(\"MODEL PERFORMANCE\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Random Forest performance\n",
        "rf_accuracy = accuracy_score(y_test, y_pred)\n",
        "rf_f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(\"\\nRandom Forest Classifier:\")\n",
        "print(f\"Overall Accuracy: {rf_accuracy:.2f}\")\n",
        "print(f\"Weighted F1-Score: {rf_f1:.2f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "\n",
        "# Baseline performance\n",
        "baseline_accuracy = accuracy_score(y_test, y_pred_baseline)\n",
        "print(f\"Baseline Accuracy (word count only): {baseline_accuracy:.2f}\")\n",
        "print(f\"Improvement over baseline: {(rf_accuracy - baseline_accuracy):.2f}\")\n",
        "\n",
        "# Feature importance\n",
        "feature_names = ['word_count', 'sentence_count', 'flesch_reading_ease']\n",
        "importances = rf_model.feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'feature': feature_names,\n",
        "    'importance': importances\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop Features:\")\n",
        "for idx, row in feature_importance_df.iterrows():\n",
        "    print(f\"{idx + 1}. {row['feature']} (importance: {row['importance']:.2f})\")\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
        "plt.title('Confusion Matrix - Quality Classification')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.savefig('data/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "print(\"Confusion matrix saved to: data/confusion_matrix.png\")\n",
        "\n",
        "# Create models directory if it doesn't exist\n",
        "if not os.path.exists('models'):\n",
        "    os.makedirs('models')\n",
        "\n",
        "# Save the model\n",
        "import pickle\n",
        "with open('models/quality_model.pkl', 'wb') as f:\n",
        "    pickle.dump({'model': rf_model, 'label_encoder': le}, f)\n",
        "print(\"Model saved to: models/quality_model.pkl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# SECTION 6: REAL-TIME ANALYSIS FUNCTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"REAL-TIME ANALYSIS FUNCTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "def analyze_url(url):\n",
        "    \"\"\"\n",
        "    Analyze a URL for content quality and duplicate detection.\n",
        "\n",
        "    Args:\n",
        "        url (str): URL to analyze\n",
        "\n",
        "    Returns:\n",
        "        dict: Analysis results including quality score and similar content\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Scrape the URL\n",
        "        print(f\"Scraping: {url}\")\n",
        "        html_content = scrape_url(url)\n",
        "\n",
        "        if not html_content:\n",
        "            return {\"error\": \"Failed to scrape URL\"}\n",
        "\n",
        "        # Parse HTML\n",
        "        parsed = parse_html_content(html_content)\n",
        "        text = parsed['body_text']\n",
        "\n",
        "        if not text.strip():\n",
        "            return {\"error\": \"No content extracted from URL\"}\n",
        "\n",
        "        # Extract features\n",
        "        cleaned_text = clean_text(text)\n",
        "        word_count = len(text.split())\n",
        "        sentence_count = count_sentences(text)\n",
        "\n",
        "        try:\n",
        "            flesch_score = textstat.flesch_reading_ease(text)\n",
        "        except:\n",
        "            flesch_score = 0\n",
        "\n",
        "        # Generate embedding\n",
        "        embedding = model.encode(text[:512])\n",
        "\n",
        "        # Predict quality\n",
        "        features = np.array([[word_count, sentence_count, flesch_score]])\n",
        "        quality_encoded = rf_model.predict(features)[0]\n",
        "        quality_label = le.inverse_transform([quality_encoded])[0]\n",
        "\n",
        "        # Check if thin content\n",
        "        is_thin = word_count < 500\n",
        "\n",
        "        # Find similar content\n",
        "        similarities = cosine_similarity([embedding], embeddings_array)[0]\n",
        "        similar_indices = np.where(similarities > 0.70)[0]  # Lower threshold for similarity\n",
        "\n",
        "        similar_content = []\n",
        "        for idx in similar_indices[:5]:  # Top 5 similar\n",
        "            similar_content.append({\n",
        "                'url': features_df.iloc[idx]['url'],\n",
        "                'similarity': float(similarities[idx])\n",
        "            })\n",
        "\n",
        "        # Sort by similarity\n",
        "        similar_content = sorted(similar_content, key=lambda x: x['similarity'], reverse=True)\n",
        "\n",
        "        # Prepare result\n",
        "        result = {\n",
        "            'url': url,\n",
        "            'title': parsed['title'],\n",
        "            'word_count': word_count,\n",
        "            'sentence_count': sentence_count,\n",
        "            'readability': round(flesch_score, 2),\n",
        "            'quality_label': quality_label,\n",
        "            'is_thin': is_thin,\n",
        "            'similar_to': similar_content\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"Analysis failed: {str(e)}\"}\n",
        "\n",
        "# Test the function with a sample URL from the dataset\n",
        "print(\"\\nTesting analyze_url() function...\")\n",
        "test_url = features_df.iloc[0]['url']\n",
        "result = analyze_url(test_url)\n",
        "\n",
        "print(\"\\nSample Analysis Result:\")\n",
        "print(json.dumps(result, indent=2))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nGenerated files:\")\n",
        "print(\"  - data/extracted_content.csv\")\n",
        "print(\"  - data/features.csv\")\n",
        "print(\"  - data/duplicates.csv\")\n",
        "print(\"  - models/quality_model.pkl\")\n",
        "print(\"  - data/confusion_matrix.png\")\n",
        "print(\"\\nYou can now use analyze_url(url) to analyze any URL in real-time!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
